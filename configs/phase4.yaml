# Phase 4 Configuration: Engram-style memory for z_dyn dynamics
# ============================================================================
# Goal: test whether a simple lookup memory improves dynamics prediction for z_dyn.
#
# Pipeline:
#   (1) scripts/30_phase4_fit_memory.py   -> fit k-means memory on train z_dyn transitions
#   (2) scripts/31_phase4_train_eval.py   -> train param/hybrid models and evaluate vs memory baseline
#
# Prereqs:
#   - outputs/phase3/zdyn.zarr and outputs/phase3/zdyn_index.parquet (exported with --split all)
#   - outputs/phase0/splits (speaker split)

data:
  zdyn_dir: ./outputs/phase3/zdyn.zarr
  zdyn_index: ./outputs/phase3/zdyn_index.parquet
  splits_dir: ./outputs/phase0/splits
  min_duration_sec: 3.0

memory:
  n_clusters: 2048
  max_fit_pairs: 300000      # number of (z_t, Δz_t) pairs used to fit k-means/statistics
  minibatch_size: 8192
  seed: 42
  output_npz: ./outputs/phase4/zdyn_memory_kmeans.npz

  # Retrieval (Engram-ish)
  topk: 8
  temperature: 1.0

  # Residual memory: store statistics of (Δz - μ_param(z)) keyed by z.
  # This is typically much more useful than trying to predict the whole Δz from memory.
  residual_enabled: true
  residual_output_npz: ./outputs/phase4/zdyn_memory_residual.npz

model:
  # Parametric dynamics p(Δz | z) mean network
  hidden_dim: 512
  n_layers: 2
  dropout: 0.0

  # Hybrid: combine parametric mean with memory mean via a learned gate
  gate_hidden_dim: 128

  # Global diagonal Gaussian scale for NLL
  min_log_sigma: -6.0
  max_log_sigma: 2.0

train:
  device: auto
  seed: 42
  batch_size: 2048
  num_workers: 2
  max_steps: 20000
  gate_steps: 5000
  lr: 5.0e-4
  weight_decay: 1.0e-4
  grad_clip_norm: 1.0
  log_every: 200

rollout_train:
  # Phase 4.5: fine-tune param dynamics with K-step rollout loss.
  enabled: true
  k: 16
  batch_size: 256
  max_steps: 5000
  segments_per_utt: 8
  lr: 2.0e-4
  grad_clip_norm: 1.0
  log_every: 100
  # Use same stabilizers as rollout eval by default
  step_alpha: 1.0
  clip_dz_l2: 5.0
  # Mixed objective to preserve teacher-forced quality while improving rollout stability:
  #   loss = rollout_weight * L_rollout + teacher_weight * L_teacher + state_weight * MSE(z_hat, z_true)
  rollout_weight: 1.0
  # Defaults slightly favor rollout stability; sweep these for a Pareto curve.
  teacher_weight: 0.1
  state_weight: 0.05

  # Scheduled sampling: with probability p, use z_true as the model input for that step
  # instead of z_hat. Linearly annealed over warmup_steps.
  sched_teacher_prob_start: 0.2
  sched_teacher_prob_end: 0.0
  sched_teacher_warmup_steps: 2000

  # Optional Gaussian noise injected into z_hat during rollout training for robustness.
  z_noise_std: 0.0

eval:
  max_batches: 200
  rollout_eval_utts: 16
  rollout_max_frames: 2000
  # Rollout stabilizers (applied to all models during rollout eval)
  rollout_step_alpha: 1.0
  rollout_clip_dz_l2: 5.0

output:
  out_dir: ./outputs/phase4
  checkpoints_dir: ./outputs/phase4/checkpoints
  train_log_jsonl: ./outputs/phase4/train_log.jsonl
  metrics_json: ./outputs/phase4/metrics.json
